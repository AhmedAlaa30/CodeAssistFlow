{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Literal, List\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Define the shared state\n",
        "# -----------------------------\n",
        "class AssistantState(TypedDict):\n",
        "    user_input: str\n",
        "    intent: Literal[\"generate\", \"explain\", \"unknown\"]\n",
        "    relevant_examples: List[str]\n",
        "    llm_response: str\n",
        "    final_output: str"
      ],
      "metadata": {
        "id": "RaCc2VgYvOCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 2. Load model\n",
        "# -----------------------------\n",
        "model_id = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "XrkrIzISvN_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3. Node: Intent Classifier\n",
        "# -----------------------------\n",
        "def classify_intent(state: AssistantState) -> AssistantState:\n",
        "    user_input = state[\"user_input\"].lower()\n",
        "    generate_keywords = [\"create\", \"write\", \"generate\", \"build\", \"make\", \"code\", \"function\"]\n",
        "    explain_keywords = [\"explain\", \"what does\", \"how does\", \"understand\", \"clarify\"]\n",
        "\n",
        "    if any(k in user_input for k in generate_keywords):\n",
        "        state[\"intent\"] = \"generate\"\n",
        "    elif any(k in user_input for k in explain_keywords):\n",
        "        state[\"intent\"] = \"explain\"\n",
        "    else:\n",
        "        state[\"intent\"] = \"unknown\"\n",
        "    return state"
      ],
      "metadata": {
        "id": "Bw3wgBbBvTcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4. Node: Retrieve Examples\n",
        "# -----------------------------\n",
        "example_bank = {\n",
        "    \"generate\": [\n",
        "        \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\"\n",
        "    ],\n",
        "    \"explain\": [\n",
        "        \"This function returns the Fibonacci number at index n using recursion.\"\n",
        "    ]\n",
        "}\n",
        "def retrieve_examples(state: AssistantState) -> AssistantState:\n",
        "    state[\"relevant_examples\"] = example_bank.get(state[\"intent\"], [])\n",
        "    return state"
      ],
      "metadata": {
        "id": "UjX05E8tvTYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5. Node: LLM Call (Updated)\n",
        "# -----------------------------\n",
        "def call_llm(state: AssistantState) -> AssistantState:\n",
        "    user_input = state[\"user_input\"].strip()\n",
        "    prompt = f\"[INST] {user_input} [/INST]\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    try:\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=False,  # Deterministic output\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        response = output_text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "        # 🧠 Trim to first meaningful block\n",
        "        if state[\"intent\"] == \"generate\":\n",
        "            match = re.search(r\"(def .*?:\\n(?:\\s+.+\\n*)+)\", response)\n",
        "            if match:\n",
        "                response = match.group(1).strip()\n",
        "        elif state[\"intent\"] == \"explain\":\n",
        "            response = response.split(\"\\n\\n\")[0].strip()\n",
        "\n",
        "        if not response:\n",
        "            response = \"⚠️ The model did not return a valid response. Try rephrasing.\"\n",
        "\n",
        "        state[\"llm_response\"] = response\n",
        "\n",
        "    except Exception as e:\n",
        "        state[\"llm_response\"] = f\"❌ Error: {str(e)}\"\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "jz5vTcxPvTWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 6. Node: Format Output\n",
        "# -----------------------------\n",
        "def format_output(state: AssistantState) -> AssistantState:\n",
        "    if state[\"intent\"] == \"generate\":\n",
        "        state[\"final_output\"] = f\"🛠️ Generated Python Code:\\n\\n{state['llm_response']}\"\n",
        "    elif state[\"intent\"] == \"explain\":\n",
        "        state[\"final_output\"] = f\"📘 Explanation:\\n\\n{state['llm_response']}\"\n",
        "    else:\n",
        "        state[\"final_output\"] = f\"💬 Response:\\n\\n{state['llm_response'] or 'Sorry, I couldn’t understand your request. Try rephrasing it.'}\"\n",
        "    return state"
      ],
      "metadata": {
        "id": "tlyirjmBvaaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 7. Build LangGraph Workflow\n",
        "# -----------------------------\n",
        "builder = StateGraph(AssistantState)\n",
        "\n",
        "builder.add_node(\"classify_intent\", classify_intent)\n",
        "builder.add_node(\"retrieve_examples\", retrieve_examples)\n",
        "builder.add_node(\"call_llm\", call_llm)\n",
        "builder.add_node(\"format_output\", format_output)\n",
        "\n",
        "builder.set_entry_point(\"classify_intent\")\n",
        "builder.add_edge(\"classify_intent\", \"retrieve_examples\")\n",
        "builder.add_edge(\"retrieve_examples\", \"call_llm\")\n",
        "builder.add_edge(\"call_llm\", \"format_output\")\n",
        "builder.add_edge(\"format_output\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "f6j9qIeBvaYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 8. CLI Interaction\n",
        "# -----------------------------\n",
        "def run_assistant():\n",
        "    print(\"🚀 LangGraph Python Assistant is ready!\")\n",
        "    while True:\n",
        "        user_input = input(\"\\n🧑‍💻 Ask your Python question (or type 'exit' to quit):\\n> \").strip()\n",
        "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"\\n👋 Goodbye! Have a great day.\")\n",
        "            break\n",
        "\n",
        "        initial_state: AssistantState = {\n",
        "            \"user_input\": user_input,\n",
        "            \"intent\": \"unknown\",\n",
        "            \"relevant_examples\": [],\n",
        "            \"llm_response\": \"\",\n",
        "            \"final_output\": \"\"\n",
        "        }\n",
        "\n",
        "        final_state = graph.invoke(initial_state)\n",
        "        print(f\"\\n{final_state['final_output']}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 9. Entry Point\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    run_assistant()\n"
      ],
      "metadata": {
        "id": "8_Yhe_hutADk",
        "outputId": "f2361ac5-e236-4804-eea2-1771d0496f5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 LangGraph Python Assistant is ready!\n",
            "\n",
            "🧑‍💻 Ask your Python question (or type 'exit' to quit):\n",
            "> Write Python code to build list func\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🛠️ Generated Python Code:\n",
            "\n",
            "def build_list(n, m):\n",
            "    list_func = []\n",
            "    for i in range(n, m+1):\n",
            "        list_func.append(i)\n",
            "    return list_func\n",
            "\n",
            "🧑‍💻 Ask your Python question (or type 'exit' to quit):\n",
            "> explain list func\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📘 Explanation:\n",
            "\n",
            "[INST] explain list\n",
            "\n",
            "🧑‍💻 Ask your Python question (or type 'exit' to quit):\n",
            "> explain list function\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🛠️ Generated Python Code:\n",
            "\n",
            "The list function in Python is used to create a list. A list in Python is a collection of items in a particular order. It is mutable, or changeable, and allows duplicate members.\n",
            "\n",
            "Here is a basic syntax of the list function:\n",
            "\n",
            "```python\n",
            "list_name = list(iterable)\n",
            "```\n",
            "\n",
            "- `iterable` is the object that you want to convert into a list.\n",
            "\n",
            "Here is an example:\n",
            "\n",
            "```python\n",
            "fruits = ['apple', 'banana', 'cherry']\n",
            "print(fruits)\n",
            "```\n",
            "\n",
            "This will output:\n",
            "\n",
            "```python\n",
            "['apple', 'banana', 'cherry']\n",
            "```\n",
            "\n",
            "You can also create an empty list:\n",
            "\n",
            "```python\n",
            "empty_list = list()\n",
            "```\n",
            "\n",
            "Or create a list from a range:\n",
            "\n",
            "```python\n",
            "numbers = list(range(10))\n",
            "```\n",
            "\n",
            "This will output:\n",
            "\n",
            "```python\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "```\n",
            "\n",
            "You can also create a list from a tuple:\n",
            "\n",
            "🧑‍💻 Ask your Python question (or type 'exit' to quit):\n",
            "> What is the difference between list comprehension and a normal for loop?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💬 Response:\n",
            "\n",
            "List comprehension is a compact way of creating a new list by performing some operation on each item in an existing list. It is a more Pythonic way of writing list comprehensions.\n",
            "\n",
            "On the other hand, a normal for loop is used to iterate over a sequence (like a list, tuple, string, etc.) or other iterable objects.\n",
            "\n",
            "Here are some differences:\n",
            "\n",
            "1. Syntax: List comprehension uses the `for` keyword followed by an expression followed by a `in` keyword, while a normal for loop uses the `for` keyword followed by a variable name in the loop statement.\n",
            "\n",
            "2. Performance: List comprehension is generally faster than a normal for loop because they are optimized for performance.\n",
            "\n",
            "3. Readability: List comprehension is more readable and easier to understand than a normal for loop.\n",
            "\n",
            "4. Use Cases: List comprehension is generally used when you want to create a new list based on an existing list. For example, you can use list comprehension to create a new list that contains the squares of all numbers in a list.\n",
            "\n",
            "5. Use with built-in functions: List comprehension can also be used with built-in functions like\n",
            "\n",
            "🧑‍💻 Ask your Python question (or type 'exit' to quit):\n",
            "> Write Python code to Explain what this function does: def triple(x): return x * 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🛠️ Generated Python Code:\n",
            "\n",
            "[INST] Write Python code to Explain what this function does: def bitwise_and(x, y): return x\n",
            "\n",
            "🧑‍💻 Ask your Python question (or type 'exit' to quit):\n",
            "> Write Python code to Explain what this function does: def triple(x): return x * 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🛠️ Generated Python Code:\n",
            "\n",
            "[INST] Write Python code to Explain what this function does: def bitwise_and(x, y): return x\n",
            "\n",
            "🧑‍💻 Ask your Python question (or type 'exit' to quit):\n",
            "> exit\n",
            "\n",
            "👋 Goodbye! Have a great day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1UYSKBeotHCg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}